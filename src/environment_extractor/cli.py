"""Command-line entry point for the environment extraction workflow."""

from __future__ import annotations

import argparse
import sys
from pathlib import Path

from .deps import ensure_available
from .pipeline import process_nc_files, streaming_from_csv


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="‰∏Ä‰ΩìÂåñ: ‰∏ãËΩΩ->ËøΩË∏™->ÁéØÂ¢ÉÂàÜÊûê")
    parser.add_argument("--csv", default="output/nc_file_urls.csv", help="Âê´s3_urlÁöÑÂàóË°®CSV")
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="ÈôêÂà∂Â§ÑÁêÜÂâçN‰∏™NCÊñá‰ª∂ (ÈªòËÆ§Â§ÑÁêÜÂÖ®ÈÉ®)",
    )
    parser.add_argument("--nc", default=None, help="Áõ¥Êé•ÊåáÂÆöÂçï‰∏™NCÊñá‰ª∂ (Ë∑≥Ëøá‰∏ãËΩΩ‰∏éËøΩË∏™)")
    parser.add_argument(
        "--tracks",
        default=None,
        help="Áõ¥Êé•ÊåáÂÆöËΩ®ËøπCSV (Ë∑≥ËøáËøΩË∏™)\nËã•‰∏é--ncÂêåÊó∂ÁªôÂá∫ÂàôÂè™ÂÅöÁéØÂ¢ÉÂàÜÊûê",
    )
    parser.add_argument("--no-clean", action="store_true", help="ÂàÜÊûêÂêé‰∏çÂà†Èô§NC")
    parser.add_argument("--keep-nc", action="store_true", help="Âêå --no-clean (ÂÖºÂÆπ)")
    parser.add_argument("--auto", action="store_true", help="Êó†ËΩ®ËøπÂàôËá™Âä®ËøêË°åËøΩË∏™")
    parser.add_argument("--search-range", type=float, default=3.0, help="ËøΩË∏™ÊêúÁ¥¢ËåÉÂõ¥")
    parser.add_argument("--memory", type=int, default=3, help="ËøΩË∏™ËÆ∞ÂøÜÊó∂Èó¥Ê≠•")
    parser.add_argument(
        "--initials",
        default=str(Path("input") / "western_pacific_typhoons_superfast.csv"),
        help="initialTracker ÂàùÂßãÁÇπCSV",
    )
    parser.add_argument(
        "--batch",
        action="store_true",
        help="‰ΩøÁî®ÊóßÁöÑÊâπÈáèÊ®°Âºè: ÂÖàÂÖ®ÈÉ®‰∏ãËΩΩ+ËøΩË∏™, ÂÜçÁªü‰∏ÄÂÅöÁéØÂ¢ÉÂàÜÊûê",
    )
    parser.add_argument(
        "--processes",
        type=int,
        default=1,
        help="Âπ∂Ë°åËøêË°åÁöÑËøõÁ®ãÊï∞ (>=1)„ÄÇÊúÄÂ§ßÂπ∂Ë°å‰ªªÂä°Êï∞‰∏éËøõÁ®ãÊï∞‰∏ÄËá¥",
    )
    parser.add_argument(
        "--concise-log",
        action="store_true",
        help="ÂêØÁî®Á≤æÁÆÄÊó•ÂøóÊ®°ÂºèÔºå‰ªÖËæìÂá∫Êñá‰ª∂ÂÆåÊàêÊÉÖÂÜµ",
    )
    return parser


def _prepare_batch_targets(
    csv_path: Path, limit: int | None, initials_csv: Path, concise_log: bool = False
) -> list[Path]:
    import pandas as pd

    from initialTracker import track_file_with_initials as it_track_file_with_initials
    from initialTracker import _load_all_points as it_load_all_points

    from .workflow_utils import (
        combine_initial_tracker_outputs,
        download_s3_public,
        extract_forecast_tag,
        sanitize_filename,
    )

    def detail(message: str) -> None:
        if not concise_log:
            print(message)

    def summary(message: str) -> None:
        print(message)

    if not csv_path.exists():
        summary(f"‚ùå CSV‰∏çÂ≠òÂú®: {csv_path}")
        sys.exit(1)

    df = pd.read_csv(csv_path)
    required_cols = {"s3_url", "model_prefix", "init_time"}
    if not required_cols.issubset(df.columns):
        missing = required_cols - set(df.columns)
        summary(f"‚ùå CSVÁº∫Â∞ëÂøÖË¶ÅÂàó: {missing}")
        sys.exit(1)

    if limit is not None:
        df = df.head(limit)

    persist_dir = Path("data/nc_files")
    persist_dir.mkdir(parents=True, exist_ok=True)
    track_dir = Path("track_single")
    track_dir.mkdir(exist_ok=True)

    if initials_csv.exists():
        initials_path = initials_csv
    else:
        fallback = Path("input/western_pacific_typhoons_superfast.csv")
        if fallback.exists():
            summary(f"‚ö†Ô∏è ÊåáÂÆöÂàùÂßãÁÇπÊñá‰ª∂‰∏çÂ≠òÂú®, ‰ΩøÁî®ÈªòËÆ§: {fallback}")
            initials_path = fallback
        else:
            summary(f"‚ùå Êâæ‰∏çÂà∞ÂàùÂßãÁÇπCSV: {initials_csv}")
            sys.exit(1)
    initials_df = it_load_all_points(initials_path)

    prepared: list[Path] = []

    def remove_nc_file(path: Path, reason: str) -> None:
        """Âà†Èô§Êó†Ê≥ïÁî®‰∫éÂêéÁª≠ÂàÜÊûêÁöÑ NC Êñá‰ª∂„ÄÇ"""
        try:
            path.unlink()
            detail(f"üßπ Â∑≤Âà†Èô§NC ({reason})")
        except FileNotFoundError:
            pass
        except Exception as exc:
            summary(f"‚ö†Ô∏è Âà†Èô§NCÂ§±Ë¥•({reason}): {exc}")
    detail(f"‚¨áÔ∏è [ÊâπÈáèÊ®°Âºè] ÈÄêÈ°π‰∏ãËΩΩ‰∏éËøΩË∏™ (limit={limit})")
    for idx, row in df.iterrows():
        s3_url = row["s3_url"]
        model_prefix = row["model_prefix"]
        init_time = row["init_time"]
        fname = Path(s3_url).name
        forecast_tag = extract_forecast_tag(fname)
        safe_prefix = sanitize_filename(model_prefix)
        safe_init = sanitize_filename(init_time.replace(":", "").replace("-", ""))
        combined_track_csv = track_dir / f"tracks_{safe_prefix}_{safe_init}_{forecast_tag}.csv"
        nc_local = persist_dir / fname
        nc_stem = nc_local.stem

        detail(f"\n[{idx+1}/{len(df)}] ‚ñ∂Ô∏è Â§ÑÁêÜ: {fname}")

        if not nc_local.exists():
            try:
                detail(f"‚¨áÔ∏è  ‰∏ãËΩΩNC: {s3_url}")
                download_s3_public(s3_url, nc_local)
            except Exception as exc:
                summary(f"‚ùå ‰∏ãËΩΩÂ§±Ë¥•: {exc}")
                continue
        else:
            detail("üì¶ Â∑≤Â≠òÂú®NCÊñá‰ª∂, Â§çÁî®")

        track_csv: Path | None = None

        if combined_track_csv.exists():
            track_csv = combined_track_csv
            detail("üó∫Ô∏è  Â∑≤Â≠òÂú®ËΩ®ËøπCSV, Ë∑≥ËøáËøΩË∏™")
        else:
            single_candidates = sorted(track_dir.glob(f"track_*_{nc_stem}.csv"))
            if len(single_candidates) == 1:
                try:
                    combined = combine_initial_tracker_outputs(single_candidates, nc_local)
                    if combined is not None and not combined.empty:
                        combined.to_csv(single_candidates[0], index=False)
                    track_csv = single_candidates[0]
                    detail("üó∫Ô∏è  ÂèëÁé∞ÂçïÊù°ËΩ®ËøπÊñá‰ª∂, Â∑≤Êõ¥Êñ∞ÂêéÁõ¥Êé•‰ΩøÁî®")
                except Exception as exc:
                    summary(f"‚ö†Ô∏è ÂçïËΩ®ËøπÊñá‰ª∂Ê†ºÂºèÊõ¥Êñ∞Â§±Ë¥•: {exc}")
            elif len(single_candidates) > 1:
                try:
                    combined = combine_initial_tracker_outputs(single_candidates, nc_local)
                    if combined is not None and not combined.empty:
                        combined.to_csv(combined_track_csv, index=False)
                        track_csv = combined_track_csv
                        detail(
                            f"üó∫Ô∏è  ÂèëÁé∞Â§öÊù°ÂçïÁã¨ËΩ®ËøπÊñá‰ª∂, Â∑≤ÂêàÂπ∂ÁîüÊàê {combined_track_csv.name}"
                        )
                except Exception as exc:
                    summary(f"‚ö†Ô∏è ÂêàÂπ∂Â∑≤ÊúâËΩ®ËøπÂ§±Ë¥•: {exc}")

        if track_csv is not None:
            prepared.append(nc_local)
            continue

        try:
            per_storm = it_track_file_with_initials(nc_local, initials_df, track_dir)
            if not per_storm:
                detail("‚ö†Ô∏è Êó†ÊúâÊïàËΩ®Ëøπ -> Âà†Èô§NC")
                remove_nc_file(nc_local, "Êó†ËΩ®Ëøπ")
                continue
            combined = combine_initial_tracker_outputs(per_storm, nc_local)
            if combined is None or combined.empty:
                detail("‚ö†Ô∏è ÂêàÂπ∂ËΩ®ËøπÂ§±Ë¥• -> Âà†Èô§NC")
                remove_nc_file(nc_local, "Êó†ËΩ®Ëøπ")
                continue

            if combined["particle"].nunique() == 1:
                single_path = Path(per_storm[0])
                combined.to_csv(single_path, index=False)
                track_csv = single_path
                detail(f"üíæ ‰øùÂ≠òÂçïÊù°ËΩ®Ëøπ: {single_path.name}")
                if combined_track_csv.exists():
                    try:
                        combined_track_csv.unlink()
                    except Exception:
                        pass
            else:
                combined.to_csv(combined_track_csv, index=False)
                track_csv = combined_track_csv
                detail(
                    f"üíæ ÂêàÂπ∂‰øùÂ≠òËΩ®Ëøπ: {combined_track_csv.name} (Âê´ {combined['particle'].nunique()} Êù°Ë∑ØÂæÑ)"
                )
        except Exception as exc:
            summary(f"‚ùå ËøΩË∏™Â§±Ë¥•: {exc}")
            remove_nc_file(nc_local, "ËøΩË∏™Â§±Ë¥•")
            continue

        if track_csv is None:
            remove_nc_file(nc_local, "Êó†ËΩ®Ëøπ")
            continue

        prepared.append(nc_local)

    if not prepared:
        summary("‚ùå Êú™ÊàêÂäüÂáÜÂ§á‰ªª‰ΩïNCÊñá‰ª∂")
        sys.exit(1)

    return prepared


def main(argv: list[str] | None = None) -> None:
    parser = build_parser()
    args = parser.parse_args(argv)

    ensure_available()

    def detail(message: str) -> None:
        if not args.concise_log:
            print(message)

    def summary(message: str) -> None:
        print(message)

    logs_root = Path("final_single_output") / "logs"

    detail("üåÄ ‰∏Ä‰ΩìÂåñÁÉ≠Â∏¶Ê∞îÊóãÂàÜÊûêÊµÅÁ®ãÂêØÂä®")
    detail("=" * 60)

    if args.nc:
        nc_path = Path(args.nc)
        if not nc_path.exists():
            summary(f"‚ùå ÊåáÂÆöNC‰∏çÂ≠òÂú®: {nc_path}")
            sys.exit(1)
        target_nc_files = [nc_path]
        detail("üì¶ ÂçïÊñá‰ª∂ÂàÜÊûêÊ®°Âºè")
    else:
        if args.batch:
            target_nc_files = _prepare_batch_targets(
                Path(args.csv), args.limit, Path(args.initials), args.concise_log
            )
            detail(f"üì¶ ÂæÖÁéØÂ¢ÉÂàÜÊûêNCÊï∞Èáè: {len(target_nc_files)}")
        else:
            detail("üöö ÂêØÁî®ÊµÅÂºèÈ°∫Â∫èÂ§ÑÁêÜ: ÊØè‰∏™NCÁã¨Á´ãÂÆåÊàê(‰∏ãËΩΩ->ËøΩË∏™->ÁéØÂ¢ÉÂàÜÊûê->Ê∏ÖÁêÜ)")
            streaming_from_csv(
                csv_path=Path(args.csv),
                limit=args.limit,
                search_range=args.search_range,
                memory=args.memory,
                keep_nc=(args.no_clean or args.keep_nc),
                initials_csv=Path(args.initials) if args.initials else None,
                processes=max(1, args.processes),
                concise_log=args.concise_log,
                logs_root=logs_root,
            )
            detail("üéØ ÊµÅÂºèÂ§ÑÁêÜÂÆåÊàê (Êó†ÈúÄËøõÂÖ•ÊâπÈáèÂêéÂ§ÑÁêÜÂæ™ÁéØ)")
            return

    process_nc_files(
        target_nc_files,
        args,
        concise_log=args.concise_log,
        logs_root=logs_root,
    )


__all__ = ["main", "build_parser"]
